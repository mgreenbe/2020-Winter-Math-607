{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 607 -- Statistical and Machine Learning -- Winter 2020\n",
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "- Present your solutions in a single Jupyter notebook `.ipynb` file.\n",
    "\n",
    "- Call the file \n",
    "<p style=\"text-align: center;font-family: monospace\">[your last name]_[your first name]_[student number]_a1.ipynb.</p>\n",
    "Do not include the `[` and `]` in the file name.\n",
    "\n",
    "- Upload the file to the appropriate dropbox on the D2L site before 23:59 on Wednesday, March 4.\n",
    "\n",
    "- You may consult with your classmates, but you must submit your own work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Edgar Anderson's <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">iris dataset</a> was made famous by <a href=\"https://en.wikipedia.org/wiki/Ronald_Fisher\">Sir Roland Fisher</a>'s analysis in his famous 1936 paper, \"<a href=\"https://digital.library.adelaide.edu.au/dspace/handle/2440/15227\">The use of multiple measurements in taxonomic problems</a>\". The dataset consists of measurements of physical characteristics -- *sepal length*, *sepal width*, *petal length*, and *petal width* -- of various species of the iris flower -- *Iris setosa*, *Iris virginica* and *Iris versicolor* -- collected with an eye to quantifying morphological variation. Nowadays, analysis of the Iris dataset is a staple of statistical pedagogy.\n",
    "\n",
    "In this exercise, we will use Bayes Theorem and the iris dataset to train a classifier.\n",
    "\n",
    "Due to its prominence, the iris dataset is included with `scikit-learn`. Load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "print(data.feature_names)\n",
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the iris dataset includes four features, we'll only use two for our Bayes classifier: petal length (feature 2) and petal width (feature 3), numbered from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2) (150,)\n"
     ]
    }
   ],
   "source": [
    "X = data.data[:, 2:]\n",
    "y = data.target\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into a training set of size 120 and a testing set of size 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2) (120,) (30, 2) (30,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_tr.shape, y_tr.shape, X_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the relative proportions of the three species of iris in the training set. Store them in a vector `p_y`. In the Bayesian parlance, these proportions represent *prior probabilities* of the three species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate `X_tr` into three subsets according to species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_y = [X_tr[y_tr == k, :] for k in [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a 2-dimensional Gaussian distribution to each of these subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0:\n",
      "-------\n",
      "mean = [1.46315789 0.24736842]\n",
      "cov = [[0.03320057 0.00800853]\n",
      " [0.00800853 0.01283073]]\n",
      "\n",
      "class 1:\n",
      "-------\n",
      "mean = [4.30238095 1.33571429]\n",
      "cov = [[0.21145761 0.0706446 ]\n",
      " [0.0706446  0.03893728]]\n",
      "\n",
      "class 2:\n",
      "-------\n",
      "mean = [5.54 2.03]\n",
      "cov = [[0.31579487 0.05723077]\n",
      " [0.05723077 0.07087179]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "means = [np.mean(x, axis=0) for x in X_tr_y]\n",
    "covs = [np.cov(x.T) for x in X_tr_y]\n",
    "\n",
    "for i, (mean, cov) in enumerate(zip(means, covs)):\n",
    "    print(f\"class {i}:\\n-------\\nmean = {mean}\\ncov = {cov}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make contour plots of the densities for each of the three species.\n",
    "\n",
    "Make a contour plot the associated *mixture density*. (Do *not* use `GaussianMixture` from `sklearn.mixture`. That's for situations in which you *don't know the class labels*. Just take an appropriate weighted sum of the densities whose contours you just plotted above.)\n",
    "\n",
    "<div style=\"background-color: pink; padding: 10px\">\n",
    "See <code>height-weight.ipynb</code> from the Session 01 for an example in the two-class case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample $x$ in the *test set*, compute the likelihoods\n",
    "$$p(x|y=0),\\quad p(x|y=1),\\quad \\text{and} \\quad p(x|y=2)\\tag{$\\star$}$$\n",
    "using the above class-conditional Gaussian distributions fit to the training data.\n",
    "Store these in a matrix `p_x_y` of shape `(30, 3)` whose `i`-th row contains of the likelihoods of `X_te[i]` as in $(\\star)$.\n",
    "\n",
    "Predict class labels for the testing set by choosing those with the highest posterior probability.\n",
    "\n",
    "Compare your predicted class labels with `y_te`. How accurate are your predictions?\n",
    "\n",
    "<div style=\"background-color: pink; padding: 10px\">\n",
    "See <code>height-weight.ipynb</code> from the Session 01 for an example in the two-class case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Another data set included with `scikit-learn` is the\n",
    "<a href=\"https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset\">diabetes dataset</a>.\n",
    "It contains measurements of ten features for each of 442 diabetes patients as well as a quantitative measurement of disease progression after one year. The goal is to predict each patient's progression in terms of their ten baseline feature measurements.\n",
    "\n",
    "1. Using `LinearRegression` from `sklearn.linear_model`, train a linear regression model on a training subset of the data. Report the mean squared error on the corresponding testing data.\n",
    "\n",
    "2. Now use 10-fold cross-validation with mean squared to estimate the prediction error of a linear regression model for the diabetes data set, as above.\n",
    "\n",
    "2. Now perform the regression using `KNeighborsRegressor` from `sklearn.neighbors`. Use 10-fold cross-validation with mean squared error to choose an optimal value of $k$. \n",
    "\n",
    "3. `KNeighborsRegressor` accepts a parameter called `weights`\n",
    "(see <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\">the docs</a>).\n",
    "Setting `weights=\"distance\"`, the regressor will weight the neighbor response values by the inverse of their distance from the point, $x$, of interest, giving neighbors closer to $x$ more influence than those farther away.\n",
    "The default is `weights=\"uniform\"`, weighting each neighbor equally.\n",
    "Use 10-fold cross-validation with mean squared error to find an optimal choice of the pair `(n_neighbors, weights)`.\n",
    "\n",
    "4. Compare the estimates of prediction error from 1, 2, 3, and 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
